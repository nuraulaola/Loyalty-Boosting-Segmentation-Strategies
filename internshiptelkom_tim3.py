# -*- coding: utf-8 -*-
"""InternshipTelkom_Tim3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zBYfxipt3Fas0KmNwm75_xqhYZOb5IH-

**Title:**
Customer Segmentation Based on Similar Complaint Patterns

**Goals:**
1.  Enhancing Customer Experience by Understanding Customer Segmentation (Juliyant)
2.  Priority Service Management Based on Customer Segmentation (Ola)
3.  Elevating Customer Loyalty Through Segmentation Understanding (Vio)

**Reference:**
https://365datascience.com/tutorials/python-tutorials/build-customer-segmentation-models/

# 1. Import Libraries
"""

# Data analysis
import pandas as pd
import numpy as np

# Text preprocessing
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from nltk.stem import LancasterStemmer
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

!pip install Sastrawi
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

import spacy
from spacy.lang.id import Indonesian
from collections import Counter

# Data visualization
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import seaborn as sns

# Machine learning
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Suppress warnings
import warnings
warnings.filterwarnings("ignore")

# Parallel process
import multiprocessing

"""# 2. Data Preparation"""

# Read CSV
url = "https://raw.githubusercontent.com/nuraulaola/Priority-Service-for-Segmented-Customers/main/data_messaging_digitalskola.csv"
df = pd.read_csv(url)
df.head()

# Rename columns
df = df.rename(columns={'Id': 'id', 'CreatedDate': 'created_date', 'Message': 'message', 'Product__c': 'product', 'Origin': 'origin', 'Category__c': 'category', 'Sub_Category__c': 'sub_category'})

# Datetime format
df['date'] = pd.to_datetime(df['created_date'])
df.drop(columns=['created_date'], inplace=True)

# Reorder columns
df = df.reindex(columns=['id', 'date'] + [col for col in df.columns if col not in ['id', 'date']])

df.head()

# Data info
df.info()

"""**Columns Description**

1. **ID:** A unique identifier for each data point.
2. **Date:** The date and time when the data was created.
3. **Message:** The text of the message that was sent by the user.
4. **Product:** The product that the user was using.
5. **Origin:** The platform that the user used to send the message.
6. **Category:** The main category of the problem that the user is experiencing.
7. **Sub-Category:** A more specific category of the problem that the user is experiencing.

# 3. Data Cleaning & Text Preprocessing
"""

# Handling null
df.dropna(subset=['message', 'product'], inplace=True)

# Groupby 'id'
grouped_df = df.groupby('id').apply(lambda x: x.reset_index(drop=True))

# Handling null
grouped_df['category'].fillna('Unknown', inplace=True)
grouped_df['sub_category'].fillna('Unknown', inplace=True)

grouped_df

grouped_df.info()

# Unique message
chunk_size = 1000
num_chunks = (len(grouped_df) - 1) // chunk_size + 1

for i in range(num_chunks):
    start_idx = i * chunk_size
    end_idx = min((i + 1) * chunk_size, len(grouped_df))

    chunk = grouped_df.iloc[start_idx:end_idx]

    unique_message_chunk = chunk['message'].value_counts().reset_index()
    unique_message_chunk.columns = ['Message', 'Count']
    print(unique_message_chunk.to_string())

# Unique category
unique_category = grouped_df['category'].value_counts().reset_index()
unique_category.columns = ['Category', 'Count']
print(unique_category.to_string())

# Model initialization
nlp = spacy.blank("id")

# Initialize Sastrawi
stopword_remover = StopWordRemoverFactory().create_stop_word_remover()
stemmer = StemmerFactory().create_stemmer()

# Stopword removal and stemming
def preprocess_text(text):
    text = stopword_remover.remove(text)
    text = stemmer.stem(text)
    return text

grouped_df['message'] = grouped_df['message'].apply(preprocess_text)

# Tokenize and lemmatize
def tokenizer_lemmatizer(text):
    doc = nlp(text)
    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]
    return tokens

grouped_df['tokenized_lemmatized_message'] = grouped_df['message'].apply(tokenizer_lemmatizer)

# Identify abbreviations
def identify_abbreviations(texts):
    abbreviations = {}
    for text in texts:
        doc = nlp(text)
        for token in doc:
            if len(token.text) <= 3:
                abbreviations[token.text.lower()] = None
    return abbreviations

abbreviations_dict = identify_abbreviations(grouped_df['message'])

# Replace abbreviations
for abbreviation in abbreviations_dict.keys():
    if abbreviation in abbreviations:
        abbreviations_dict[abbreviation] = abbreviations[abbreviation]

def replace_abbreviations(text, abbreviations_dict):
    for abbreviation, expansion in abbreviations_dict.items():
        if expansion is not None:
            text = text.replace(abbreviation, expansion)
    return text

grouped_df['message'] = grouped_df['message'].apply(lambda x: replace_abbreviations(x, abbreviations_dict))

grouped_df

"""# 4. Exploratory Data Analysis"""

# Product distribution
df['product'].value_counts().plot(kind='bar')
plt.title('Distribution of Products')
plt.xlabel('Product')
plt.ylabel('Frequency')
plt.show()

product_counts = df['product'].value_counts()
product_dict = product_counts.to_dict()

# Generate wordcloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(product_dict)

# Display wordcloud
plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Product Distribution Word Cloud')
plt.show()

# Date distribution
df['date'].value_counts().plot(kind='bar')
plt.title('Distribution of Date')
plt.xlabel('Date')
plt.ylabel('Frequency')
plt.show()

# Origin distribution
df['origin'].value_counts().plot(kind='bar')
plt.title('Distribution of Origin')
plt.xlabel('Origin')
plt.ylabel('Frequency')
plt.show()

# Category distribution
df['category'].value_counts().plot(kind='bar')
plt.title('Distribution of Category')
plt.xlabel('Catagory')
plt.ylabel('Frequency')
plt.show()

category_counts = df['category'].value_counts()
category_dict = category_counts.to_dict()

# Generate wordcloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(category_dict)

# Display wordcloud
plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Category Distribution Word Cloud')
plt.show()

# Top 10 categories
top_categories = df['category'].value_counts().head(10)
top_categories.plot(kind='bar')
plt.title('Distribution of Top 10 Categories')
plt.xlabel('Category')
plt.ylabel('Frequency')
plt.show()

# Sub Category distribution
plt.figure(figsize=(12, 6))
df['sub_category'].value_counts().plot(kind='bar')
plt.title('Distribution of Sub Category')
plt.xlabel('Sub Category')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

sub_category_counts = df['sub_category'].value_counts()
sub_category_dict = sub_category_counts.to_dict()

# Generate wordcloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(sub_category_dict)

# Display wordcloud
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Sub-Category Distribution Word Cloud')
plt.show()

# Top 10 sub categories
top_sub_categories = df['sub_category'].value_counts().head(10)
top_sub_categories.plot(kind='bar')
plt.title('Distribution of Top 10 Sub Categories')
plt.xlabel('Sub Category')
plt.ylabel('Frequency')
plt.show()

# Definisikan data keluhan pelanggan
customer_complaints = [
    {"complaint": "Produk ini rusak saat tiba", "severity": 3},
    {"complaint": "Layanan pelanggan sangat lambat", "severity": 7},
    {"complaint": "Saya tidak puas dengan kualitas produk", "severity": 5},
    {"complaint": "Saya tidak menerima paket saya", "severity": 8},
    {"complaint": "Baterai produk ini cepat habis", "severity": 6}
]

# Fungsi untuk mengelompokkan keluhan berdasarkan tingkatan
def categorize_complaints(complaints):
    categories = {
        "Low": [],
        "Medium": [],
        "High": [],
        "Critical": []
    }
    for complaint in complaints:
        severity = complaint["severity"]
        if severity <= 3:
            categories["Low"].append(complaint)
        elif severity <= 6:
            categories["Medium"].append(complaint)
        elif severity <= 9:
            categories["High"].append(complaint)
        else:
            categories["Critical"].append(complaint)
    return categories

# Panggil fungsi untuk mengelompokkan keluhan
categorized_complaints = categorize_complaints(customer_complaints)

# Tampilkan hasil
for category, complaints in categorized_complaints.items():
    print(f"{category} Severity:")
    for complaint in complaints:
        print(f"- {complaint['complaint']}")
    print()

# Contoh DataFrame dengan kolom pesan
data = {'Message': ['Halo, apa kabar?', 'Ini adalah pesan yang lebih panjang.', 'Saya ingin menanyakan sesuatu.']}
df = pd.DataFrame(data)

# Pengukuran panjang pesan dalam karakter
df['Message_Length'] = df['Message'].str.len()

# Definisi ambang batas untuk klasifikasi
threshold = 30

# Klasifikasi berdasarkan panjang pesan
df['Message_Class'] = df['Message_Length'].apply(lambda x: 'Short Message' if x <= threshold else 'Long Message')

print(df)

"""# 5. RFM Calculation and Analysis
1. Recency: How recently have they made a purchase?
2. Frequency: How often have they bought something?
3. Severity of Complaints (Modified Monetary): What is the severity or impact level of their complaints?

## 5.1 Recency Analysis
"""

# Latest date
latest_date = grouped_df['date'].max()
print("Latest Transaction or Complaint Date:", latest_date)

# Date distribution
df['date'].hist(bins=30, figsize=(10, 6))
plt.title('Distribution of Transaction or Complaint Dates')
plt.xlabel('Date')
plt.ylabel('Frequency')
plt.show()

"""## 5.2 Frequency Analysis"""

# Frequency distribution
grouped_df.hist(bins=20, figsize=(10, 6))
plt.title('Complaint Frequency')
plt.xlabel('Date')
plt.ylabel('Number of Customers')
plt.show()

"""# 6. Segmentation Model Development

# 7. Interpretation and Evaluation
"""